{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent diffusion models on HR images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "Latent Diffusion Models (LDMs) are a class of deep generative models that combine the strengths of autoencoders and diffusion models. They perform the diffusion process in a lower-dimensional latent space, which makes them computationally efficient while generating high-quality data.\n",
    "\n",
    "This training will be performed with the Galaxy Zoo Dataset :\n",
    "\n",
    "The Galaxy Zoo Dataset is a citizen science project initiated as part of the Zooniverse platform, designed to involve the public in the morphological classification of galaxies. It consists of images sourced from large-scale sky surveys like the Sloan Digital Sky Survey (SDSS), Hubble Space Telescope, and others.\n",
    "\n",
    "This notebook is made to train the autoencoder and then the diffusion model from the latent space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "2. Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torchvision.transforms import (Compose, ToTensor,\n",
    "                                    Lambda, Resize, CenterCrop,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomRotation, Normalize)\n",
    "from packaging import version\n",
    "\n",
    "torch_version = version.parse(torch.__version__)\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5.0\"):\n",
    "    from torch.amp import autocast, GradScaler\n",
    "else:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Custom Libraries\n",
    "from libGalaxyZooDataset import *\n",
    "from libDDPM import *\n",
    "from libGPU_torch_utils import *\n",
    "#from libAutoEncoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function combining MSE and SSIM\n",
    "# SSMI = Structural Similarity Index Measure \n",
    "# (https://en.wikipedia.org/wiki/Structural_similarity)\n",
    "\n",
    "def loss_with_ssmi(reconstructed, original):\n",
    "    mse_loss = F.mse_loss(reconstructed, original)\n",
    "    ssim_loss = 1 - ssim(reconstructed, original,\n",
    "                         data_range=1, size_average=True)  # SSIM loss (inverted)\n",
    "    return mse_loss + 0.1 * ssim_loss  # Weight SSIM loss by a factor, adjust as needed\n",
    "\n",
    "def version_aware_autocast(device):\n",
    "    \"\"\"\n",
    "    Create an autocast context manager based on the PyTorch version.\n",
    "    Depending on the version, the `device_type` argument may or \n",
    "    may not be supported.\n",
    "    \n",
    "    Args:\n",
    "        device (str): Device to use (cpu or cuda).\n",
    "\n",
    "    Returns:\n",
    "        autocast: Context manager for mixed precision training.\n",
    "    \"\"\"\n",
    "    if torch_version >= version.parse(\"2.5.0\"):\n",
    "        return autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\")\n",
    "    else:\n",
    "        return autocast()\n",
    "\n",
    "def visualize_latent_space(latent_vectors, labels, epoch, output_dir, strdate):\n",
    "    \"\"\"\n",
    "    Visualizes the latent space using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        latent_vectors (np.array): Latent vectors from the encoder.\n",
    "        labels (list): Corresponding labels for each vector.\n",
    "        epoch (int): Current epoch number (for saving).\n",
    "        output_dir (str): Directory to save the plot.\n",
    "        strdate (str): Timestamp string for filenames.\n",
    "    \"\"\"\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'x': reduced_vectors[:, 0],\n",
    "        'y': reduced_vectors[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='x', y='y', hue='label', palette='viridis', data=df, alpha=0.8)\n",
    "    plt.title(f'Latent Space Visualization at Epoch {epoch}')\n",
    "    plt.legend(title='Labels', loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, f\"{strdate}_latent_space_epoch_{epoch:03}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the parameters from the config file\n",
    "\n",
    "All the parameters are save into a single file : 'param_GZ2.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA RTX 3000 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the parameters file\n",
    "parameters_file = 'param_GZ2.json'\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "if os.path.exists(parameters_file):\n",
    "# Load the parameters from the JSON file\n",
    "    with open(parameters_file, 'r') as json_file:\n",
    "        parameters = json.load(json_file)\n",
    "\n",
    "    # Now you can access the parameters using dictionary-style access\n",
    "    no_train = parameters['no_train']\n",
    "    verbose = parameters['verbose']\n",
    "    test_model = parameters['test_model']\n",
    "    batch_size = parameters['batch_size']\n",
    "    latent_dim = parameters['latent_dim']\n",
    "    n_epochs = parameters['n_epochs']\n",
    "    lr = parameters['lr']\n",
    "    n_steps = parameters['n_steps']\n",
    "    min_beta = parameters['min_beta']\n",
    "    max_beta = parameters['max_beta']\n",
    "    nb_gal4training = parameters['nb_gal4training']\n",
    "    nb_gal2plot = parameters['nb_gal2plot']\n",
    "    plot_subset = parameters['plot_subset']\n",
    "    output_model = parameters['output_model']\n",
    "    ref_column_file = parameters['ref_column_file']\n",
    "    mapping_file = parameters['mapping_file']\n",
    "    DATALOCATION_DIR = parameters['DATALOCATION_DIR']\n",
    "    DDPM_dir = parameters['DDPM_dir']\n",
    "    output_dir = parameters['output_dir']\n",
    "    plot_reconstruction = parameters['plot_reconstruction']        \n",
    "    save_freq = parameters['save_freq']\n",
    "    val_freq = parameters['val_freq'] \n",
    "\n",
    "    # Carbon followup\n",
    "    carbon_estimation = parameters['carbon_estimation']\n",
    "    carbon_log_dir = parameters['carbon_log_dir']\n",
    "    carbon_log_file = parameters['carbon_log_file']\n",
    "    training_log_file = parameters['training_log_file']\n",
    "\n",
    "else:\n",
    "    print(f\"File {parameters_file} does not exist\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data preparation\n",
    "\n",
    "Working with the Galaxie Zoo 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0 name: NVIDIA RTX 3000 Ada Generation Laptop GPU\n",
      "Valid GPU references: ['cuda:0']\n",
      "GPU cuda:0 may be currently in use...\n",
      "\n",
      "--------------------------------------------------\n",
      "Read the file with all the galaxies...\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "mydevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = setup_device(mydevice)\n",
    "\n",
    "\n",
    "if not os.path.exists('tensorboard_logs'):\n",
    "    os.makedirs('tensorboard_logs')\n",
    "tensorboard_log_dir = os.path.join(DDPM_dir, 'tensorboard_logs_LDM')\n",
    "writer = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "\n",
    "### Load the data ###\n",
    "print(\"\\n\"+\"-\"*50)\n",
    "print(\"Read the file with all the galaxies...\")\n",
    "print(\"-\"*50+\"\\n\")\n",
    "\n",
    "# Check if the file exists and is not empty\n",
    "if not os.path.exists(os.path.join(DATALOCATION_DIR, ref_column_file)):\n",
    "    print(f\"File {ref_column_file} does not exist in {DATALOCATION_DIR}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if os.path.getsize(os.path.join(DATALOCATION_DIR, ref_column_file)) <= 0:\n",
    "    print(f\"File {ref_column_file} is empty\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df_galaxy_zoo_header = pd.read_csv(os.path.join(DATALOCATION_DIR,\n",
    "                                                ref_column_file), delimiter=',',\n",
    "                                                header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Prepare a subset of data\n",
    "\n",
    "To speed up the training, we are only using a subset of data. \n",
    "One can select the data according to the morphology of the galaxies.\n",
    "\n",
    "Then, this cell prepare the Dataloaders, one for the train dataset and one for the validation dataset.\n",
    "\n",
    "We also define a transformation to increase the dataset size and improve the quality of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho = 'S'\n",
    "morpho_gal = {'S': 'spiral', 'E': 'elliptical', 'A': 'erregular'}\n",
    "\n",
    "# Display the spiral galaxies (gz2_class starting with 'S')\n",
    "print(\"Selection of spiral galaxies from the dataset:\")\n",
    "df_sub_morpho = df_galaxy_zoo_header[df_galaxy_zoo_header['gz2_class'].str.startswith(morpho)]\n",
    "\n",
    "# Load the mapping file\n",
    "df_mapping = pd.read_csv(os.path.join(DATALOCATION_DIR, mapping_file),\n",
    "                            delimiter=',',\n",
    "                            header=0)\n",
    "\n",
    "# Match where dr7objid in df_spiral = objid in df_mapping to add asset_id to the df_spiral\n",
    "df_sub_morpho = pd.merge(df_sub_morpho, df_mapping, left_on='dr7objid', right_on='objid')\n",
    "\n",
    "print(f\"Number of {morpho_gal[morpho]} galaxies in the dataset: {df_sub_morpho.shape[0]}\")\n",
    "if nb_gal4training < 0 or nb_gal4training > df_sub_morpho.shape[0]:\n",
    "    nb_gal2training = df_sub_morpho.shape[0]\n",
    "else:   \n",
    "    nb_gal4training = min(nb_gal4training, df_sub_morpho.shape[0])\n",
    "\n",
    "print(f\"Number of {morpho_gal[morpho]} galaxies for training: {nb_gal4training}\")\n",
    "# Create a subset of galaxies to plot if needed\n",
    "df_sub_morpho_subset = df_sub_morpho.sample(n=nb_gal4training, random_state=SEED)\n",
    "\n",
    "# Exit if the subset is empty\n",
    "if df_sub_morpho_subset.empty:\n",
    "    print(\"The subset of galaxies is empty\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#\n",
    "# PREPARE DATA LOADERS\n",
    "#\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((64, 64)),\n",
    "    #CenterCrop(40),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(20),\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: x.float())\n",
    "])\n",
    "\n",
    "transform_tanh = Compose([\n",
    "    Resize((64, 64)),\n",
    "    #CenterCrop(40),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(20),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Define the dataset and the dataloader\n",
    "dataset = GalaxyZooDataset(df_sub_morpho_subset, DATALOCATION_DIR,\n",
    "                        transform=transform)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "validation_split = 0.2  # 20% for validation\n",
    "validation_size = int(validation_split * dataset_size)\n",
    "training_size = dataset_size - validation_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset,\n",
    "                                          [training_size, validation_size])\n",
    "\n",
    "# Custom collate function to handle the variable image sizes or empty tensors\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                        shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Free up memory\n",
    "del df_galaxy_zoo_header, df_sub_morpho, df_mapping, df_sub_morpho_subset\n",
    "del dataset\n",
    "del train_dataset\n",
    "del val_dataset\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "if verbose:\n",
    "    print(\"Show the first batch of images...\")\n",
    "    show_first_batch(train_loader, save2file=True,\n",
    "                        filename='output/first_batch_GZ2.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOENCODER - Training phase\n",
    "\n",
    "The purpose of the autoencoder in the Latent Diffusion Model (LDM) is to learn a compact latent representation of the input images. This latent representation will then serve as the space where the diffusion process operates, significantly reducing computational complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        \"\"\"\n",
    "        Encoder: Extracts latent features from input images.\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, latent_dim),\n",
    "        )\n",
    "        self.latent_norm = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.latent_norm(z)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        \"\"\"\n",
    "        Decoder: Reconstructs images from latent features.\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * 4 * 4),  # Match encoder's downsampled size\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Unflatten(1, (512, 4, 4)),  \n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Output values between [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_training = False\n",
    "\n",
    "if autoencoder_training:\n",
    "\n",
    "    # bypass n_epochs\n",
    "    n_epochs = 10\n",
    "\n",
    "    autoencoder = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(autoencoder.parameters(), lr=1e-4) \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                        patience=5, factor=0.5)\n",
    "\n",
    "\n",
    "    # The following code block is used to handle mixed precision training\n",
    "    # It could be put into a function to avoid code duplication TODO\n",
    "    if torch_version < version.parse(\"2.5.0\"):\n",
    "        scaler = torch.amp.GradScaler(init_scale=65536.0, growth_interval=2000)\n",
    "    else:\n",
    "        scaler = GradScaler()\n",
    "\n",
    "    num_epochs = n_epochs\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "\n",
    "    # For plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # To visualize latent space\n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    reconstruction_errors = []\n",
    "\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    strdate = f\"{datenow}_autoencoder_training\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        autoencoder.train()\n",
    "        running_loss = 0.0\n",
    "        step_bar = tqdm(train_loader, leave=False,\n",
    "                            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "                            colour=\"#005500\")\n",
    "        for _, batch in enumerate(step_bar):\n",
    "            if batch is None:  # Skip batches with missing files\n",
    "                continue\n",
    "\n",
    "            x0 = batch[0].to(device)   # Input images\n",
    "            batch_labels = batch[2]\n",
    "            #with autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n",
    "            with version_aware_autocast(device):\n",
    "                encoded = autoencoder.encoder(x0)\n",
    "                decoded = autoencoder.decoder(encoded)\n",
    "                loss = loss_with_ssmi(decoded, x0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Estimate validation loss every `val_freq` epochs\n",
    "        if (epoch + 1) % val_freq != 0: \n",
    "            autoencoder.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            latent_vectors_list = []\n",
    "            labels_list = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, batch in enumerate(val_loader):\n",
    "                    if batch is None:\n",
    "                        continue\n",
    "\n",
    "                    x0 = batch[0].to(device)\n",
    "                    batch_labels = batch[2]\n",
    "\n",
    "                #with autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n",
    "                with version_aware_autocast(device):\n",
    "                    encoded = autoencoder.encoder(x0)\n",
    "                    decoded = autoencoder.decoder(encoded)\n",
    "                    loss = loss_with_ssmi(decoded, x0)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Compute reconstruction error per sample\n",
    "                    errors = torch.mean((x0 - decoded) ** 2, dim=[1, 2, 3])  # MSE per sample\n",
    "                    reconstruction_errors.extend(errors.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                    # Collect latent vectors and labels\n",
    "                    latent_vectors_list.append(encoded.detach().cpu().numpy())\n",
    "                    labels_list.extend(batch_labels)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Plot reconstruction error distribution\n",
    "        if plot_reconstruction:\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.hist(reconstruction_errors, bins=50, color='skyblue', edgecolor='black')\n",
    "            plt.title(f'Histogram of Reconstruction Errors at Epoch {epoch}')\n",
    "            plt.xlabel(f'Reconstruction Error at Epoch {epoch}')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(output_dir,f'{strdate}_reconstruction_errors_{epoch:03}.png'))\n",
    "            #plt.show()\n",
    "\n",
    "            # Visualize original and reconstructed images\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_batch = next(iter(val_loader))[0].to(device)\n",
    "                with autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n",
    "                    reconstructed_batch = autoencoder(sample_batch)\n",
    "\n",
    "            sample_batch = sample_batch.cpu()\n",
    "            reconstructed_batch = reconstructed_batch.cpu()\n",
    "\n",
    "            n = 6  # Number of images to display\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            for i in range(n):\n",
    "                # Original images\n",
    "                ax = plt.subplot(2, n, i + 1)\n",
    "                plt.imshow(np.transpose(sample_batch[i].numpy(), (1, 2, 0)))\n",
    "                plt.title(\"Original\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Reconstructed images\n",
    "                ax = plt.subplot(2, n, i + 1 + n)\n",
    "                plt.imshow(np.transpose(reconstructed_batch[i].numpy(), (1, 2, 0)))\n",
    "                plt.title(\"Reconstructed\")\n",
    "                plt.suptitle(f\"Original and Reconstructed Images at Epoch {epoch}\")\n",
    "                plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir,\n",
    "                                    f\"{strdate}_reconstructed_images_{epoch:03}.png\"))\n",
    "            #plt.show()\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(autoencoder.state_dict(),\n",
    "                        os.path.join(output_dir,\n",
    "                                    f\"{strdate}_best_autoencoder.pth\"))\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "        # Save the model every save_freq epochs\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            torch.save(autoencoder.state_dict(),\n",
    "                        os.path.join(output_dir,\n",
    "                                    f\"{strdate}_autoencoder_epoch_{epoch+1}.pth\"))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Concatenate all latent vectors\n",
    "        latent_vectors = np.concatenate(latent_vectors_list, axis=0)\n",
    "        labels = np.array(labels_list)\n",
    "        \n",
    "        subset_size = 1000\n",
    "        if len(latent_vectors) > subset_size:\n",
    "            indices = np.random.choice(len(latent_vectors), size=subset_size,\n",
    "                                    replace=False)\n",
    "            latent_vectors_subset = latent_vectors[indices]\n",
    "            labels_subset = labels[indices]\n",
    "        else:\n",
    "            latent_vectors_subset = latent_vectors\n",
    "            labels_subset = labels\n",
    "\n",
    "        # Visualize latent space\n",
    "        visualize_latent_space(latent_vectors_subset, labels_subset, epoch, \n",
    "                            output_dir, strdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Visualisation of the LOSS for Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if autoencoder_training:\n",
    "\n",
    "    # After training\n",
    "    print(\"Training complete! Plotting training and validation loss...\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir,\n",
    "                                f\"{strdate}_training_validation_loss.png\"))\n",
    "    plt.show()\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Diffusion Process\n",
    "\n",
    "At that stage, the autoencoder is trained. \n",
    "\n",
    "We now reload the best latent model which will be used as input of the diffusion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LatentDiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent Diffusion Model for image generation\n",
    "\n",
    "    Args:\n",
    "        network (nn.Module): Neural network for predicting noise\n",
    "        n_steps (int): Number of steps in the diffusion process\n",
    "        min_beta (float): Minimum value of beta\n",
    "        max_beta (float): Maximum value of beta\n",
    "        device (str): Device to run the model on\n",
    "        latent_dim (int): Dimension of the latent space\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, n_steps=1000, min_beta=1e-4, max_beta=0.02, \n",
    "                 device=None, latent_dim=128):\n",
    "        super(LatentDiffusionModel, self).__init__()\n",
    "        self.network = network.to(device)\n",
    "        self.n_steps = n_steps\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Diffusion hyper param'\n",
    "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) \\\n",
    "                                        + 1e-6 for i in range(len(self.alphas))]).to(device)\n",
    "\n",
    "    def q_t(self, z_0, t, eps=None):\n",
    "        \"\"\"\n",
    "        Compute the diffusion process at time t \n",
    "        on the latent space : \n",
    "        q(z_t| z_0) = N(z_t | mu_t, sigma_t) with \n",
    "\n",
    "        beta_i = betas[i]\n",
    "        alpha_bar_t = prod_{i=1}^{t} (1 - beta_i)\n",
    "\n",
    "        mu_t = sqrt(1 - alpha_bar_t) * z_0\n",
    "        sigma_t = sqrt(alpha_bar_t) * I\n",
    "\n",
    "        z_t = mu_t + sigma_t * eps\n",
    "\n",
    "        Args:\n",
    "            z_0 (torch.Tensor): Initial latent vector\n",
    "            t (int): Time step\n",
    "            eps (torch.Tensor): Noise tensor\n",
    "        \"\"\"\n",
    "        # alpha_bar_t: [batch_size] after indexing\n",
    "        alpha_bar_t = self.alpha_bars[t]  # [batch_size]\n",
    "        alpha_bar_t = alpha_bar_t.unsqueeze(1)  # [batch_size, 1]\n",
    "        # Broadcast alpha_bar_t to match z_0: [batch_size, latent_dim]\n",
    "        # Since latent_dim is the same for all samples, we can expand:\n",
    "        alpha_bar_t = alpha_bar_t.expand(-1, self.latent_dim)  # [batch_size, latent_dim]\n",
    "\n",
    "        z_t = (alpha_bar_t.sqrt() * z_0) + ((1 - alpha_bar_t).sqrt() * eps)\n",
    "        # z_t: [batch_size, latent_dim]\n",
    "        return z_t\n",
    "\n",
    "    def predict_eps(self, z_t, t):\n",
    "        \"\"\"\n",
    "        Predict the noise added at timestep t\n",
    "\n",
    "        \"\"\"\n",
    "        return self.network(z_t, t)\n",
    "\n",
    "    def compute_loss(self, z_0, t):\n",
    "        \"\"\"\n",
    "        Loss calculation for the diffusion model\n",
    "        \n",
    "        Formula:\n",
    "        L = ||eps_theta - eps||^2   with eps ~ N(0, I)\n",
    "\n",
    "        Args:\n",
    "            z_0 (torch.Tensor): Initial latent vector\n",
    "            t (int): Time step\n",
    "        \"\"\"\n",
    "        eps = torch.randn_like(z_0)        # [batch_size, latent_dim]\n",
    "        z_t = self.q_t(z_0, t, eps)        # [batch_size, latent_dim]\n",
    "        eps_theta = self.predict_eps(z_t, t)  # [batch_size, latent_dim]\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        return loss_fn(eps_theta, eps)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, z_t, t):\n",
    "        \"\"\"\n",
    "        Reverse diffusion process\n",
    "        p(z_0 | z_t) = N(z_0 | mu_t, sigma_t) with\n",
    "\n",
    "        beta_i = betas[i]\n",
    "        alpha_bar_t = prod_{i=1}^{t} (1 - beta_i)\n",
    "\n",
    "        Args:\n",
    "            z_t (torch.Tensor): Latent vector at time t\n",
    "            t (int): Time step\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract scalars per batch element\n",
    "        beta_t = self.betas[t]          # [batch_size]\n",
    "        alpha_t = self.alphas[t]        # [batch_size]\n",
    "        alpha_bar_t = self.alpha_bars[t]# [batch_size]\n",
    "\n",
    "        # alpha_bar_prev (for t>0), else 1.0\n",
    "        alpha_bar_prev = torch.where(\n",
    "            t > 0,\n",
    "            self.alpha_bars[(t-1).clamp(min=0)],\n",
    "            torch.ones_like(alpha_bar_t)\n",
    "        )  # [batch_size]\n",
    "\n",
    "        # Expand all to [batch_size, latent_dim]\n",
    "        beta_t = beta_t.unsqueeze(1).expand(-1, self.latent_dim)\n",
    "        alpha_t = alpha_t.unsqueeze(1).expand(-1, self.latent_dim)\n",
    "        alpha_bar_t = alpha_bar_t.unsqueeze(1).expand(-1, self.latent_dim)\n",
    "        alpha_bar_prev = alpha_bar_prev.unsqueeze(1).expand(-1, self.latent_dim)\n",
    "\n",
    "        eps_theta = self.predict_eps(z_t, t)  # [batch_size, latent_dim]\n",
    "\n",
    "        noise = torch.randn_like(z_t)         # [batch_size, latent_dim]\n",
    "        sigma_t = ((1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t).sqrt() # [batch_size, latent_dim]\n",
    "\n",
    "        z_prev = (z_t - (1 - alpha_t).sqrt() * eps_theta) / alpha_t.sqrt()\n",
    "\n",
    "        # Add noise if t>0\n",
    "        mask = (t > 0).unsqueeze(1).expand(-1, self.latent_dim)  # [batch_size, latent_dim]\n",
    "        z_prev = z_prev + torch.where(mask, sigma_t * noise, torch.zeros_like(z_t))\n",
    "        return z_prev\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from the model\n",
    "    \n",
    "        \"\"\"\n",
    "        z_t = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
    "\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            t_tensor = torch.full((num_samples, 1), t,\n",
    "                                  dtype=torch.long).to(self.device)\n",
    "            z_t = self.p_sample(z_t, t_tensor)\n",
    "\n",
    "        return z_t\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode_latent(self, z_0, decoder):\n",
    "        \"\"\"\n",
    "        Decode the latent vector to an image\n",
    "\n",
    "        Args:\n",
    "            z_0 (torch.Tensor): Latent vector\n",
    "            decoder (nn.Module): pre-trained decoder\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded image\n",
    "        \"\"\"\n",
    "        return decoder(z_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing the autoencoder \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_epochs_diffusion = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "best_ae_path = os.path.join(output_dir, \"20241128_084322_autoencoder_training_best_autoencoder.pth\")\n",
    "\n",
    "# Chargement de l'autoencodeur pré-entraîné\n",
    "autoencoder = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "autoencoder.load_state_dict(torch.load(best_ae_path))\n",
    "autoencoder.eval()  # Mode évaluation pour éviter toute modification des poids\n",
    "print(f\"Best autoencoder reloaded from file {best_ae_path}.\")\n",
    "\n",
    "class NoisePredictor(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(NoisePredictor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        # z_t is [batch_size, latent_dim]\n",
    "        return self.net(z_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the diffusion model on the latent space\n",
    "\n",
    "Instead of training the diffusion model on the images, the latent diffusion model is training the diffusion network on the latent space. \n",
    "\n",
    "The model used in the diffusion model is often a U-Net. But here, for the sake of this example, we are just using a FCN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model_training = True\n",
    "if diffusion_model_training:\n",
    "\n",
    "    noise_predictor = NoisePredictor(latent_dim=latent_dim).to(device)\n",
    "\n",
    "    diffusion_model = LatentDiffusionModel(\n",
    "        network=noise_predictor,\n",
    "        n_steps=n_steps,\n",
    "        min_beta=min_beta,\n",
    "        max_beta=max_beta,\n",
    "        latent_dim=latent_dim,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    optimizer_diffusion = torch.optim.Adam(diffusion_model.network.parameters(), lr=learning_rate)\n",
    "    scheduler_diffusion = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_diffusion, mode='min',\n",
    "                                                        patience=5, factor=0.5)\n",
    "\n",
    "    if torch_version < version.parse(\"2.5.0\"):\n",
    "        scaler = torch.amp.GradScaler(init_scale=65536.0, growth_interval=2000)\n",
    "    else:\n",
    "        scaler = GradScaler()\n",
    "\n",
    "    # Diffusion model training\n",
    "    for epoch in range(n_epochs_diffusion):\n",
    "        diffusion_model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs_diffusion}\", leave=False)\n",
    "        False\n",
    "        for images in progress_bar:\n",
    "            x0 = images[0].to(device)\n",
    "\n",
    "            with torch.no_grad():  # L'autoencodeur n'est pas modifié\n",
    "                z_0 = autoencoder.encoder(x0)\n",
    "\n",
    "\n",
    "            t = torch.randint(0, n_steps, (z_0.size(0),), device=device)\n",
    "            with version_aware_autocast(device):\n",
    "                loss = diffusion_model.compute_loss(z_0, t)\n",
    "\n",
    "            optimizer_diffusion.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer_diffusion)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * z_0.size(0)\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        tqdm.write(f\"Epoch [{epoch+1}/{n_epochs_diffusion}], Training Loss: {epoch_loss:.4f}\")\n",
    "        progress_bar.set_postfix({\"Train loss\": f\"{epoch_loss:.4f}\"})\n",
    "\n",
    "        scheduler_diffusion.step(epoch_loss)\n",
    "\n",
    "        diffusion_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images in val_loader:\n",
    "                x0 = images[0].to(device)\n",
    "                z_0 = autoencoder.encoder(x0)\n",
    "                t = torch.randint(0, n_steps, (z_0.size(0),), device=device)\n",
    "\n",
    "                with autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n",
    "                    loss = diffusion_model.compute_loss(z_0, t)\n",
    "                val_loss += loss.item() * z_0.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        tqdm.write(f\"Epoch [{epoch+1}/{n_epochs_diffusion}], Validation Loss: {val_loss:.4f}\")\n",
    "        progress_bar.set_postfix({\"Val loss\": f\"{val_loss:.4f}\"})\n",
    "\n",
    "        # Log the losses to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        # Save the model every save_freq epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_path = os.path.join(output_dir, f\"diffusion_model_epoch_{epoch + 1}.pth\")\n",
    "            torch.save(diffusion_model.state_dict(), save_path)\n",
    "\n",
    "    # Sauvegarde du modèle de diffusion\n",
    "    torch.save(diffusion_model.state_dict(), os.path.join(output_dir, \"diffusion_model.pth\"))\n",
    "    print(\"Modèle de diffusion sauvegardé sous 'diffusion_model.pth'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torchvision.transforms import (Compose, ToTensor,\n",
    "                                    Lambda, Resize, CenterCrop,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomRotation, Normalize)\n",
    "from packaging import version\n",
    "\n",
    "torch_version = version.parse(torch.__version__)\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5.0\"):\n",
    "    from torch.amp import autocast, GradScaler\n",
    "else:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Custom Libraries\n",
    "from libGalaxyZooDataset import *\n",
    "from libDDPM import *\n",
    "from libGPU_torch_utils import *\n",
    "from libAutoEncoder4LDM import * \n",
    "from libLDM import *  \n",
    "\n",
    "\n",
    "\n",
    "class NoisePredictor(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(NoisePredictor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        # z_t is [batch_size, latent_dim]\n",
    "        return self.net(z_t)\n",
    "\n",
    "\n",
    "#from libLDM import LatentDiffusionModel\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_samples(diffusion_model, autoencoder, num_samples):\n",
    "    \"\"\"\n",
    "    Generate samples using the trained diffusion model and autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        diffusion_model (LatentDiffusionModel): The trained diffusion model.\n",
    "        autoencoder (Autoencoder): The trained autoencoder.\n",
    "        num_samples (int): Number of samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Generated images.\n",
    "    \"\"\"\n",
    "    # Initialize latent vectors z_t with the same spatial dimensions as the encoder output\n",
    "    latent_shape = (num_samples, latent_dim, 16, 16)  # Adjust 8x8 to match your latent space size\n",
    "    print(f\"Latent dimensions: {latent_dim}\")\n",
    "    #z_t = torch.randn(latent_shape).to(device)  # Random noise as input\n",
    "    z_t = torch.randn(num_samples, latent_dim, device=device)\n",
    "    print(f\"Initial z_t shape: {z_t.shape}\")\n",
    "    # Reverse diffusion process\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_tensor = torch.full((num_samples,), t, dtype=torch.long, device=device)\n",
    "        print(f\"t_tensor shape at step {t}: {t_tensor.shape}\")\n",
    "        z_t = diffusion_model.p_sample(z_t, t_tensor)\n",
    "        print(f\"z_t shape at step {t}: {z_t.shape}\")\n",
    "    \n",
    "    # Decode the final latent vectors into images\n",
    "    generated_images = autoencoder.decoder(z_t)\n",
    "    return generated_images\n",
    "output_dir = 'output'\n",
    "latent_dim = 512\n",
    "n_steps = 1000\n",
    "min_beta = 1e-4\n",
    "max_beta = 0.02\n",
    "best_ae_path = os.path.join(output_dir, \"20241128_084322_autoencoder_training_best_autoencoder.pth\")\n",
    "\n",
    "device = setup_device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Chargement de l'autoencodeur pré-entraîné\n",
    "autoencoder = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "autoencoder.load_state_dict(torch.load(best_ae_path))\n",
    "autoencoder.eval()  # Mode évaluation pour éviter toute modification des poids\n",
    "print(f\"Best autoencoder reloaded from file {best_ae_path}.\")\n",
    "\n",
    "noise_predictor = NoisePredictor(latent_dim=latent_dim).to(device)\n",
    "\n",
    "diffusion_model = LatentDiffusionModel(\n",
    "    network=noise_predictor,\n",
    "    n_steps=n_steps,\n",
    "    min_beta=min_beta,\n",
    "    max_beta=max_beta,\n",
    "    latent_dim=latent_dim,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "diffusion_model.load_state_dict(torch.load(os.path.join(output_dir, \"LDM_diffusion_model_best.pth\")))\n",
    "\n",
    "print(\"Diffusion model loaded from file.\")\n",
    "\n",
    "# Generate new samples\n",
    "generated_img = generate_samples(diffusion_model, autoencoder, num_samples=16)\n",
    "\n",
    "# Plot the generated images\n",
    "import torchvision.utils as vutils\n",
    "grid = vutils.make_grid(generated_img, nrow=4, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
